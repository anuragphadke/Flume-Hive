<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<?asciidoc-toc?>
<?asciidoc-numbered?>

<article lang="en">
<articleinfo>
    <title>Flume Cookbook</title>
    <date>November 16 2010</date>
    <author>
        <firstname>flume-dev@cloudera.org</firstname>
    </author>
    <authorinitials>F</authorinitials>
<revhistory><revision><revnumber>0.9.3-SNAPSHOT</revnumber><date>November 16 2010</date><authorinitials>F</authorinitials></revision></revhistory>
</articleinfo>
<section id="_introduction">
<title>Introduction</title>
<simpara>Flume is a distributed, reliable, and available service for
efficiently collecting, aggregating, and moving large amounts of log
data.  Being highly configurable and very extensible means that there
are many options and thus many decisions that need to be made by an
operator.  This document is a "cookbook" with "recipes" for getting
Flume up and running quickly without being overwhelmed with all the
details.</simpara>
<simpara>This document starts with some basic tips on experimenting with Flume
nodes, and then three stories about setting up agents for different
kinds of sources.  Finally we&#8217;ll do a quick setup for collectors and
how to troubleshoot distributed deployments.</simpara>
<itemizedlist>
<listitem>
<simpara>
Basic debugging with one-shot Flume nodes
</simpara>
</listitem>
<listitem>
<simpara>
Flume agents for Apache web server logging
</simpara>
</listitem>
<listitem>
<simpara>
Flume agents for syslog / syslog-ng logging
</simpara>
</listitem>
<listitem>
<simpara>
Flume agents for scribe logging


</simpara>
</listitem>
</itemizedlist>
</section>
<section id="_trying_out_flume_sources_and_sinks">
<title>Trying out Flume sources and sinks</title>
<simpara>First this section will describe some basic tips for testing sources,
sinks and logical nodes.</simpara>
<section id="_testing_sources">
<title>Testing sources</title>
<simpara>Testing sources is a straightforward process.  The <literal>flume</literal> script has
a <literal>dump</literal> command that allows you test data source configuration by
displaying data at the console.</simpara>
<screen>flume dump source</screen>
<note><simpara><literal>source</literal> needs to be a single command line argument, so you may
need to add <emphasis>quotes</emphasis> or "quotes" to the argument if it has quotes or
parenthesis in them.  Using single quotes allow you to use unescaped
double quotes in the configuration argument. (ex: <literal>'text("/tmp/foo")'</literal>
or <literal>"text(\"/tmp/foo\")"</literal>).</simpara></note>
<simpara>Here are some simple examples to try:</simpara>
<screen>$ flume dump console
$ flume dump 'text("/path/to/file")'
$ flume dump 'file("/path/to/file")'
$ flume dump syslogTcp
$ flume dump 'syslogUdp(5140)'
$ flume dump 'tail("/path/to/file")'
$ flume dump 'tailDir("path/to/dir", "fileregex")'
$ flume dump 'rpcSource(12346)'</screen>
<simpara>Under the covers, this dump command is actually running the <literal>flume
node_nowatch</literal> command with some extra command line parameters.</simpara>
<screen>flume node_nowatch -1 -s -n dump -c "dump: $1 | console;"</screen>
<simpara>Here&#8217;s a summary of what the options mean.</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0"><tgroup cols="2"><colspec colwidth="15*"/><colspec colwidth="85*"/><tbody valign="top">
<row>
<entry>
<simpara>
<literal>-1</literal> 
</simpara>
</entry>
<entry>
<simpara>
one shot execution.  This makes the node instance not use the
heartbeating mechanism to get a config.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>-s</literal> 
</simpara>
</entry>
<entry>
<simpara>
starts the Flume node without starting the node&#8217;s http status
web server.  If the status web server is started, a Flume node&#8217;s
status server will keep the process alive even if in one-shot mode.
If the -s flag is specified along with one-shot mode (-1), the Flume
node will exit after all logical nodes complete.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>-c "node:src|snk;"</literal> 
</simpara>
</entry>
<entry>
<simpara>
Starts the node with the given configuration
 definition.  NOTE: If not using -1, this will be invalidated
 upon the first heartbeat to the master.
</simpara>
</entry>
</row>
<row>
<entry>
<simpara>
<literal>-n node</literal> 
</simpara>
</entry>
<entry>
<simpara>
gives the node the physical name node.
</simpara>
</entry>
</row>
</tbody></tgroup></informaltable>
<simpara>You can get info on all of the Flume node commands by using this command:</simpara>
<screen>$ flume node -h</screen>
</section>
<section id="_testing_sinks_and_decorators">
<title>Testing sinks and decorators</title>
<simpara>Now that you can test sources, there is only one more step necessary
to test arbitrary sinks and decorators from the command line.  A sink
requires data to consume so some common sources used to generate test
data include synthetic datasets (<literal>asciisynth</literal>), the console
(<literal>console</literal>), or files (<literal>text</literal>).</simpara>
<simpara>For example, you can use a synthetic source to generate 1000 events,
each of 100 "random" bytes data to the console. You could use a text
source to read a file like /etc/services, or you could use the console
as a source and interactively enter lines of text as events:</simpara>
<screen>$ flume node_nowatch -1 -s -n dump -c 'dump: asciisynth(1000,100) | console;'
$ flume node_nowatch -1 -s -n dump -c 'dump: text("/etc/services") | console;'
$ flume node_nowatch -1 -s -n dump -c 'dump: console | console;'</screen>
<simpara>You can also use decorators on the sinks you specify.  For example,
you could rate limit the amount of data that pushed from the source to
sink by inserting a delay decorator.  In this case, the <emphasis>delay</emphasis>
decorator waits 100ms before sending each synthesized event to the
console.</simpara>
<screen>$ flume node_nowatch -1 -s -n dump -c 'dump: asciisynth(1000,100) | { delay(100) =&gt; console};'</screen>
<simpara>Using the command line, you can send events via the direct best-effort
(BE) or disk-failover (DFO) agents.  The example below uses the
<literal>console</literal> source so you can interactively generate data to send in BE
and DFO mode to collectors.</simpara>
<note><simpara>Flume node_nowatch must be used when piping data in to a Flume
node&#8217;s console source.  The watchdog program does not forward stdin.</simpara></note>
<screen>$ flume node_nowatch -1 -s -n dump -c 'dump: console | agentBESink("collectorHost");'
$ flume node_nowatch -1 -s -n dump -c 'dump: console | agentDFOSink("collectorHost");'</screen>
<warning><simpara>Since these nodes are executed with configurations entered at
the command line and never contact the master, they cannot use the
automatic chains or logical node translations.  Currently, the
acknowledgements used in E2E mode go through the master piggy-backed
on node-to-master heartbeats.  Since this mode does not heartbeat, E2E
mode should not be used.</simpara></warning>
<simpara>Console sources are useful because we can pipe data into Flume
directly.  The next example pipes data from a program into Flume which
then delivers it.</simpara>
<screen>$ &lt;external process&gt; | flume node_nowatch -1 -s -n foo -c 'foo:console|agentBESink("collector");'</screen>
<simpara>Ideally, you could write data to a named pipe and just have Flume read
data from a named pipe using <literal>text</literal> or <literal>tail</literal>.  Unfortunately, this
version of Flume&#8217;s <literal>text</literal> and <literal>tail</literal> are not currently compatible with
named pipes in a Linux environment.  However, you could pipe data to a
Flume node listening on the stdin console:</simpara>
<screen>$ tail -f namedpipe | flume node_nowatch -1 -s -n foo -c 'foo:console|agentBESink;'</screen>
<simpara>Or you can use the exec source to get its output data:</simpara>
<screen>$ flume node_nowatch -1 -s -n bar -c 'bar:exec("cat pipe")|agentBESink;'</screen>
</section>
<section id="_monitoring_nodes">
<title>Monitoring nodes</title>
<simpara>While outputting data to a console or to a logfile is an effective way
to verify data transmission, Flume nodes provide a way to monitor the
state of sources and sinks.  This can be done by looking at the node&#8217;s
report page.  By default, you can navigate your web browser to the
nodes TCP 35862 port.  (<ulink url="http://node:35862/">http://node:35862/</ulink>)</simpara>
<simpara>This page shows counter information about all of the logical nodes on
the physical node as well as some basic machine metric information.</simpara>
<tip><simpara>If you have multiple physical nodes on a single machine, there
may be a port conflict.  If you have the auto-find port option on
(<literal>flume.node.http.autofindport</literal>), the physical node will increment the
port number until it finds a free port it can bind to.</simpara></tip>
</section>
</section>
<section id="_using_flume_agents_for_apache_2_x_web_server_logging">
<title>Using Flume Agents for Apache 2.x Web Server Logging</title>
<simpara>To connect Flume to Apache 2.x servers, you will need to</simpara>
<simpara># Configure web log file permissions</simpara>
<simpara># Tail the web logs or use piped logs to enable Flume to get data from
  the web server.</simpara>
<simpara>This section will step through basic setup on default Ubuntu Lucid and
default CentOS 5.5 installations.  Then it will describe various ways
of integrating Flume.</simpara>
<section id="_if_you_are_using_centos_red_hat_apache_servers">
<title>If you are using CentOS / Red Hat Apache servers</title>
<simpara>By default, CentOS&#8217;s Apache writes weblogs to files owned by <literal>root</literal>
and in group <literal>adm</literal> in 0644 (rw-r&#8212;r--) mode.  Flume is run as the
<literal>flume</literal> user, so the Flume node is able to read the logs.</simpara>
<simpara>Apache on CentOS/Red Hat servers defaults to writing logs to two
files:</simpara>
<screen>/var/log/httpd/access_log
/var/log/httpd/error_log</screen>
<simpara>The simplest way to gather data from these files is to tail the files
by configuring Flume nodes to use Flume&#8217;s <literal>tail</literal> source:</simpara>
<screen>tail("/var/log/httpd/access_log")
tail("/var/log/httpd/error_log")</screen>
</section>
<section id="_if_you_are_using_ubuntu_servers_apache_servers">
<title>If you are using Ubuntu servers Apache servers</title>
<simpara>By default, Ubuntu writes weblogs to files owned by <literal>root</literal> and in
group <literal>adm</literal> in 0640 (rw-r-----) mode.  Flume is run as the <literal>flume</literal>
user and by default will <emphasis role="strong">not</emphasis> be able to tread the files.  One
approach to allow the <literal>flume</literal> user to read the files is to add it to
the <literal>adm</literal> group.</simpara>
<simpara>Apache servers on Ubuntu defaults to writing logs to three files:</simpara>
<screen>/var/log/apache2/access.log
/var/log/apache2/error.log
/var/log/apache2/other_vhosts_access.log</screen>
<simpara>The simplest way to gather data from these files is by configuring
Flume nodes to use Flume&#8217;s <literal>tail</literal> source:</simpara>
<screen>tail("/var/log/apache2/access.log")
tail("/var/log/apache2/error.log")
tail("/var/log/apache2/other_vhosts_access.log")</screen>
</section>
<section id="_getting_log_entries_from_piped_log_files">
<title>Getting log entries from Piped Log files</title>
<simpara>The Apache 2.x&#8217;s documentation
(<ulink url="http://httpd.apache.org/docs/2.2/logs.html">http://httpd.apache.org/docs/2.2/logs.html</ulink>) describes using piped
logfile with the <literal>CustomLog</literal> directive.  Their example uses
<literal>rotatelogs</literal> to periodically write data to new files with a given
prefix.  Here are some example directives that could be in the
<literal>httpd.conf</literal>/<literal>apache2.conf</literal> file.</simpara>
<screen>LogFormat "%h %l %u %t \"%r\" %&gt;s %b" common
CustomLog "|/usr/sbin/rotatelogs /var/log/apache2/foo_access_log 3600" common</screen>
<tip><simpara>In Ubuntu Lucid these directives are in
<literal>/etc/apache2/sites-available/default</literal>. In CentOS 5.5, these
directives are in <literal>/etc/httpd/conf/httpd.conf</literal>.</simpara></tip>
<simpara>These directives configure Apache to write log files in
<literal>/var/log/apache2/foo_access_log.xxxxx</literal> every hour (3600 seconds)
using the "common" log format.</simpara>
<simpara>You can use Flume&#8217;s <literal>tailDir</literal> source to read all files without
modifing the Apache settings:</simpara>
<screen>tailDir("/var/log/apache2/", "foo_access_log.*")</screen>
<simpara>The first argument is the directory, and then the second is a regex
that should match against the file name.  <literal>tailDir</literal> will watch the dir
and tail all files that have matching file names.</simpara>
</section>
<section id="_using_piped_logs">
<title>Using piped logs</title>
<simpara>Instead of writing data to disk and then having Flume read it, you can
have Flume ingest data directly from Apache.  To do so, modify the web
server&#8217;s parameters and use its piped log feature by adding some
directives to Apache&#8217;s configuration:</simpara>
<screen>CustomLog "|flume node_nowatch -1 -s -n apache -c \'apache:console|agentBESink(\"collector\");\'" common</screen>
<screen>CustomLog "|flume node_nowatch -1 -s -n apache -c \'apache:console|agentDFOSink(\"collector\");\'" common</screen>
<warning><simpara>By default, CentOS does not have the Java required by the
Flume node in user <literal>root</literal>'s path.  You can use <literal>alternatives</literal> to
create a managed symlink in <literal>/usr/bin/</literal> for the java executable.</simpara></warning>
<simpara>Using piped logs can be more efficient, but is riskier because Flume
can deliver messages without saving on disk.  Doing this, however,
increases the probabiltiy of event loss.  From a security point of
view, this Flume node instance runs as Apache&#8217;s user which is often
<literal>root</literal> according to the Apache manual.</simpara>
<note><simpara>You could configure the one-shot mode node to deliver data
directly to a collector.  This can only be done at the best effort or
disk-failover level.</simpara></note>
<simpara>The prior examples use Flume nodes in one-shot mode which runs without
contacting a master.  Unfortunately, it means that one-shot mode
cannot directly use the automatic chains or the end-to-end (E2E)
reliablity mode.  This is because the automatic chains are generated
by the master and because E2E mode currently delivers acknowledgements
through the master.</simpara>
<simpara>However, you can have a one-shot Flume node deliver data to a Flume
local node daemon where the reliable E2E mode can be used.  In this
setup we would have the following Apache directive:</simpara>
<screen>CustomLog "|flume node_nowatch -1 -s -n apache -c \'apache:console|agentBESink(\"localhost\", 12345);\'" common</screen>
<simpara>Then you can have a Flume node setup to listen with the following
configuration:</simpara>
<screen>node : rpcSource(12345) | agentE2ESink("collector");</screen>
<simpara>Since this daemon node is connected to the master, it can use the
auto*Chains.</simpara>
<screen>node : rpcSource(12345) | autoE2EChain;</screen>
<note><simpara>End-to-end mode attempts to ensure the deliver of the data that
enters the E2E sink.  In this one-shot-node to reliable-node scenario,
data is not safe it gets to the E2E sink.  However, since this is a
local connection, it should only fail when the machine or processes
fails.  The one-shot node can be set to disk failover (DFO) mode in
order to reduce the chance of message loss if the daemon node&#8217;s
configuration changes.</simpara></note>
</section>
</section>
<section id="_flume_agents_for_syslog_data">
<title>Flume Agents for Syslog data</title>
<simpara><literal>syslog</literal> is the standard unix single machine logging service.  Events
are generally emitted as lines with a time stamp, "facility" type,
priority, and message.  Syslog can be configured to send data to
remote destinations.  The default syslog remote delivery was
originally designed to provide best effort delivery service.  Today,
there are several more advanced syslog services that deliver messages
with improved reliability (TCP connections with memory buffering on
failure).  The reliability guarantees however are one hop and weaker
than Flume&#8217;s more reliable delivery mechanism.</simpara>
<simpara>This section describes collecting syslog data using two methods.  The
first part describes a file tailing approach.  The latter parts
describe syslog system configuration guidance that enables directly
feeding Flume&#8217;s <literal>syslog*</literal> sources.</simpara>
<section id="_tailing_files">
<title>Tailing files</title>
<simpara>The quickest way to record syslog messages is to tail syslog generated
log files.  These files generally live in <literal>/var/log</literal>.</simpara>
<simpara>Some examples include:</simpara>
<screen>/var/log/auth.log
/var/log/messages
/var/log/syslog
/var/log/user.log</screen>
<simpara>These files could be tailed by Flume nodes with tail sources:</simpara>
<screen>tail("/var/log/auth.log")
tail("/var/log/messages")
tail("/var/log/syslog")
tail("/var/log/user.log")</screen>
<simpara>Depending on your system configuration, there may be permissions
issues when accessing these files from the Flume node process.</simpara>
<note><simpara>Red Hat/CentOS systems default to writing log files owned by
root, in group root, and with 0600 (-rw-------) permissions. Flume
could be run as root, but this is not advised because Flume can be
remotely configured to execute arbitrary programs.</simpara></note>
<note><simpara>Ubuntu systems default to writing logs files owned by syslog, in
group adm, and with 0640 (-rw-r-----) permissions.  By adding the user
"flume" to group "adm", a Flume node running user "flume" should be
able to read the syslog generated files.</simpara></note>
<note><simpara>When tailing files, the time when the event is read is used as the
time stamp.</simpara></note>
</section>
<section id="_delivering_syslog_events_via_sockets">
<title>Delivering Syslog events via sockets</title>
<simpara>The original syslog listens to the <literal>/dev/log</literal> named pipe, and can be
configured to listen on UDP port
514. (<ulink url="http://tools.ietf.org/search/rfc5424">http://tools.ietf.org/search/rfc5424</ulink>). More advanced versions
(rsyslog, syslog-ng) can send and recieve over TCP and may do
in-memory queuing/buffering. For example, syslog-ng and rsyslog can
optionally use the default UDP port 514 or use TCP port 514 for better
recovery options.</simpara>
<note><simpara>By default only superusers can listen on on UDP/TCP ports 514.
Unix systems usually only allow ports &lt;1024 to be bound by
superusers. While Flume can run as superuser, from a security stance
this is not advised.  The examples provide directions to route to the
user-bindable port 5140.</simpara></note>
<simpara>For debugging syslog configurations, you can just use <emphasis>flume dump</emphasis>
with syslog sources.  This command outputs received syslog data to the
console.  To test if syslog data is coming in to the proper port you
can run this command from the command line:</simpara>
<screen>$ flume dump 'syslogUdp(5140)'</screen>
<simpara>This will dump all incoming events to the console.</simpara>
<simpara>If you are satisfied with your connection, you can have a Flume node
run on the machine configure its sink for the reliability level you
desire.</simpara>
<simpara>Using a <literal>syslog*</literal> Flume source will save the entire line of event
data, use the timestamp found in the original data, extract a <literal>host</literal>,
and attempt to extract a service from the syslog line.  All of these
map to a Flume event&#8217;s fields except for <literal>service</literal> so this is added as
extra metadata field to each event (this is a convention with
syslog defined in RFC).</simpara>
<simpara>So, a syslog entry whose body is this:</simpara>
<screen>Sep 14 07:57:24 blitzwing dhclient: bound to 192.168.126.212 -- renewal in 710 seconds.</screen>
<simpara>will have the Flume event body:</simpara>
<screen>Sep 14 07:57:24 blitzwing dhclient: bound to 192.168.126.212 -- renewal in 710 seconds.</screen>
<simpara>The event will also translated the "Sep 14 07:57:24" date+time data so
that it will be bucketable.  Since this date does not have a year, it
assumes the current year and since it has no timezone it assumes the
local timezone.  The host field should be "blitzwing", and the
optional "service" metadata field will contain "dhclient".</simpara>
<section id="_configuring_literal_syslogd_literal">
<title>Configuring <literal>syslogd</literal></title>
<simpara>The original syslog is <literal>syslogd</literal>.  It is configured by an
<literal>/etc/syslog.conf</literal> file.  Its format is fairly simple.</simpara>
<simpara>Syslog recieves messages and then sends to out to different facilities
that have associated names
(<ulink url="http://tools.ietf.org/search/rfc5424#section-6.2">http://tools.ietf.org/search/rfc5424#section-6.2</ulink>).</simpara>
<simpara>The <literal>/etc/syslog.conf</literal> file essentially contains lists of facilities
and "actions".  These "actions" are destinations such as regular
files, but can also be named pipes, consoles, or remote machines.  One
can specify a remote machine by prefixing an <emphasis>@</emphasis> symbol in front the
destination host machine.  If no port is specified, events are sent
via UDP port 514.</simpara>
<simpara>The example below specifies delivery to machine localhost on port
5140.</simpara>
<screen>user.*     @localhost:5140</screen>
<simpara>A Flume node daemon running on this machine would have a <literal>syslogUdp</literal>
source listening for new log data.</simpara>
<screen>host-syslog : syslogUdp(5140) | autoE2EChain ;</screen>
</section>
<section id="_configuring_literal_rsyslog_literal">
<title>Configuring <literal>rsyslog</literal></title>
<simpara><literal>rsyslog</literal> is a more advanced drop-in replacement for syslog and the
default syslog system used by Ubuntu systems.  It supports basic
filtering, best effort delivery, and queuing for handling one-hop
downstream failures.</simpara>
<simpara><literal>rsyslog</literal> actually extends the syslog configuration file format.
Similar to regular <literal>syslogd</literal> you can send data to a remote machine on
listening on UDP port 514 (standard syslog port).</simpara>
<screen>*.*   @remotehost</screen>
<simpara>Moreover, <literal>rsyslog</literal> also allows you to use the more reliable TCP
protocol to send data to a remote host listening on TCP port 514.  In
<literal>rsyslog</literal> configurations, an <emphasis>@@</emphasis> prefix dictates the use of TCP.</simpara>
<screen>*.*  @@remotehost</screen>
<simpara>Similarly, you can also append a suffix port number to have it deliver
to a particular port.  In this example, events are delivered to
localhost TCP port 5140.</simpara>
<screen>*.*  @@localhost:5140</screen>
<simpara>Assuming you have a Flume node daemon running on the local host, you
can capture syslog data by adding a logical node with the following
configuration:</simpara>
<screen>host-syslog : syslogTcp(5140) | autoE2EChain ;</screen>
</section>
<section id="_configuring_literal_syslog_ng_literal">
<title>Configuring <literal>syslog-ng</literal></title>
<simpara>Syslog-ng is another common replacement for the default syslog logging
system.  Syslog-ng has a different configuration file format but
essentially gives the operator the ability to send syslog data from
different facilities to different remote destinations.  TCP or UDP can
be used.</simpara>
<simpara>Here is an example of modifications to a <literal>syslog-ng.conf</literal> (often found
in <literal>/etc/syslog-ng/</literal>) file.</simpara>
<screen>## set up logging to loghost (which is flume)
destination loghost {
        tcp("localhost" port(5140));
};

# send everything to loghost, too
log {
        source(src);
        destination(loghost);
};</screen>
<simpara>Assuming you have a Flume node daemon running on the local host, you
can capture syslog data by adding a logical node with the following
configuration:</simpara>
<screen>host-syslog : syslogTcp(5140) | autoE2EChain ;</screen>
</section>
</section>
</section>
<section id="_logging_scribe_events_to_a_flume_agent">
<title>Logging Scribe Events to a Flume Agent</title>
<simpara>Flume can emulate a downstream node for applications that log using
Scribe.  Scribe is Facebook&#8217;s open source log aggregation framework.
It has a simple API and uses Thrift as its core network transport
mechanism.  Flume uses the same Thrift IDL file and can listen for
data provided by Scribe sources.</simpara>
<simpara>Scribe comes with an example application called <literal>scribe_cat</literal>.  It acts
like the unix <literal>cat</literal> program but sends data to a downstream host with a
specified "category".  Scribe by default uses TCP port 1463.</simpara>
<simpara>You can configure a Flume node to listen for incoming Scribe traffic
by creating a logical node that uses the <literal>scribe</literal> source.  We can then
assign an arbitrary sink to the node.  In the example below, the
Scribe nodes receives events, send its events to both the console and
an automatically-assigned end-to-end agent which delivers the events
downstream to its collector pools.</simpara>
<screen>scribe: scribe | [console, autoE2EChain];</screen>
<simpara>Assuming that this node communicates with the master, and that the
node is on <literal>localhost</literal>, we can use the <literal>scribe_cat</literal> program to send
data to Flume.</simpara>
<screen>$ echo "foo" | scribe_cat localhost testcategory</screen>
<simpara>When the Scribe source accepts the Scribe events, it converts Scribe&#8217;s
category information into a new Flume event metadata entry, and then
delivers the event to its sinks.  Since Scribe does not include time
metadata, the timestamp of the created Flume event will be the arrival
time of the Scribe event.</simpara>
<literallayout class="monospaced">     ______
    / ___//_  ______  ____
   / /_/ / / / /    \/ __/
  / __/ / /_/ / / / / __/
 / / /_/\____/_/_/_/\__/
/_/ Distributed Log Collection.</literallayout>
</section>
</article>
