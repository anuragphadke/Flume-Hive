<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1"><title>Flume Cookbook</title><link rel="stylesheet" href="docbook.css" type="text/css"><meta name="generator" content="DocBook XSL Stylesheets V1.75.2"></head><body><div style="clear:both; margin-bottom: 4px"></div><div align="center"><a href="index.html"><img src="images/home.png" alt="Documentation Home"></a></div><span class="breadcrumbs"><div class="breadcrumbs"><span class="breadcrumb-node">Flume Cookbook</span></div></span><div lang="en" class="article" title="Flume Cookbook"><div class="titlepage"><div><div><h2 class="title"><a name="id467712"></a>Flume Cookbook</h2></div><div><div class="author"><h3 class="author"><span class="firstname">flume-dev@cloudera.org</span></h3></div></div><div><div class="revhistory"><table border="1" width="100%" summary="Revision history"><tr><th align="left" valign="top" colspan="3"><b>Revision History</b></th></tr><tr><td align="left">Revision 0.9.3-SNAPSHOT</td><td align="left">November 16 2010</td><td align="left">F</td></tr></table></div></div></div><hr></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#_introduction">1. Introduction</a></span></dt><dt><span class="section"><a href="#_trying_out_flume_sources_and_sinks">2. Trying out Flume sources and sinks</a></span></dt><dd><dl><dt><span class="section"><a href="#_testing_sources">2.1. Testing sources</a></span></dt><dt><span class="section"><a href="#_testing_sinks_and_decorators">2.2. Testing sinks and decorators</a></span></dt><dt><span class="section"><a href="#_monitoring_nodes">2.3. Monitoring nodes</a></span></dt></dl></dd><dt><span class="section"><a href="#_using_flume_agents_for_apache_2_x_web_server_logging">3. Using Flume Agents for Apache 2.x Web Server Logging</a></span></dt><dd><dl><dt><span class="section"><a href="#_if_you_are_using_centos_red_hat_apache_servers">3.1. If you are using CentOS / Red Hat Apache servers</a></span></dt><dt><span class="section"><a href="#_if_you_are_using_ubuntu_servers_apache_servers">3.2. If you are using Ubuntu servers Apache servers</a></span></dt><dt><span class="section"><a href="#_getting_log_entries_from_piped_log_files">3.3. Getting log entries from Piped Log files</a></span></dt><dt><span class="section"><a href="#_using_piped_logs">3.4. Using piped logs</a></span></dt></dl></dd><dt><span class="section"><a href="#_flume_agents_for_syslog_data">4. Flume Agents for Syslog data</a></span></dt><dd><dl><dt><span class="section"><a href="#_tailing_files">4.1. Tailing files</a></span></dt><dt><span class="section"><a href="#_delivering_syslog_events_via_sockets">4.2. Delivering Syslog events via sockets</a></span></dt><dd><dl><dt><span class="section"><a href="#_configuring_literal_syslogd_literal">4.2.1. Configuring <code class="literal">syslogd</code></a></span></dt><dt><span class="section"><a href="#_configuring_literal_rsyslog_literal">4.2.2. Configuring <code class="literal">rsyslog</code></a></span></dt><dt><span class="section"><a href="#_configuring_literal_syslog_ng_literal">4.2.3. Configuring <code class="literal">syslog-ng</code></a></span></dt></dl></dd></dl></dd><dt><span class="section"><a href="#_logging_scribe_events_to_a_flume_agent">5. Logging Scribe Events to a Flume Agent</a></span></dt></dl></div><div class="section" title="1. Introduction"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_introduction"></a>1. Introduction</h2></div></div></div><p>Flume is a distributed, reliable, and available service for
efficiently collecting, aggregating, and moving large amounts of log
data.  Being highly configurable and very extensible means that there
are many options and thus many decisions that need to be made by an
operator.  This document is a "cookbook" with "recipes" for getting
Flume up and running quickly without being overwhelmed with all the
details.</p><p>This document starts with some basic tips on experimenting with Flume
nodes, and then three stories about setting up agents for different
kinds of sources.  Finally we&#8217;ll do a quick setup for collectors and
how to troubleshoot distributed deployments.</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
Basic debugging with one-shot Flume nodes
</li><li class="listitem">
Flume agents for Apache web server logging
</li><li class="listitem">
Flume agents for syslog / syslog-ng logging
</li><li class="listitem">
Flume agents for scribe logging


</li></ul></div></div><div class="section" title="2. Trying out Flume sources and sinks"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_trying_out_flume_sources_and_sinks"></a>2. Trying out Flume sources and sinks</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_testing_sources">2.1. Testing sources</a></span></dt><dt><span class="section"><a href="#_testing_sinks_and_decorators">2.2. Testing sinks and decorators</a></span></dt><dt><span class="section"><a href="#_monitoring_nodes">2.3. Monitoring nodes</a></span></dt></dl></div><p>First this section will describe some basic tips for testing sources,
sinks and logical nodes.</p><div class="section" title="2.1. Testing sources"><div class="titlepage"><div><div><h3 class="title"><a name="_testing_sources"></a>2.1. Testing sources</h3></div></div></div><p>Testing sources is a straightforward process.  The <code class="literal">flume</code> script has
a <code class="literal">dump</code> command that allows you test data source configuration by
displaying data at the console.</p><pre class="screen">flume dump source</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p><code class="literal">source</code> needs to be a single command line argument, so you may
need to add <span class="emphasis"><em>quotes</em></span> or "quotes" to the argument if it has quotes or
parenthesis in them.  Using single quotes allow you to use unescaped
double quotes in the configuration argument. (ex: <code class="literal">'text("/tmp/foo")'</code>
or <code class="literal">"text(\"/tmp/foo\")"</code>).</p></td></tr></table></div><p>Here are some simple examples to try:</p><pre class="screen">$ flume dump console
$ flume dump 'text("/path/to/file")'
$ flume dump 'file("/path/to/file")'
$ flume dump syslogTcp
$ flume dump 'syslogUdp(5140)'
$ flume dump 'tail("/path/to/file")'
$ flume dump 'tailDir("path/to/dir", "fileregex")'
$ flume dump 'rpcSource(12346)'</pre><p>Under the covers, this dump command is actually running the <code class="literal">flume
node_nowatch</code> command with some extra command line parameters.</p><pre class="screen">flume node_nowatch -1 -s -n dump -c "dump: $1 | console;"</pre><p>Here&#8217;s a summary of what the options mean.</p><div class="horizontal"><table style="border: none;"><colgroup><col><col></colgroup><tbody valign="top"><tr><td style="" valign="top">
<p>
<code class="literal">-1</code> 
</p>
</td><td style="" valign="top">
<p>
one shot execution.  This makes the node instance not use the
heartbeating mechanism to get a config.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">-s</code> 
</p>
</td><td style="" valign="top">
<p>
starts the Flume node without starting the node&#8217;s http status
web server.  If the status web server is started, a Flume node&#8217;s
status server will keep the process alive even if in one-shot mode.
If the -s flag is specified along with one-shot mode (-1), the Flume
node will exit after all logical nodes complete.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">-c "node:src|snk;"</code> 
</p>
</td><td style="" valign="top">
<p>
Starts the node with the given configuration
 definition.  NOTE: If not using -1, this will be invalidated
 upon the first heartbeat to the master.
</p>
</td></tr><tr><td style="" valign="top">
<p>
<code class="literal">-n node</code> 
</p>
</td><td style="" valign="top">
<p>
gives the node the physical name node.
</p>
</td></tr></tbody></table></div><p>You can get info on all of the Flume node commands by using this command:</p><pre class="screen">$ flume node -h</pre></div><div class="section" title="2.2. Testing sinks and decorators"><div class="titlepage"><div><div><h3 class="title"><a name="_testing_sinks_and_decorators"></a>2.2. Testing sinks and decorators</h3></div></div></div><p>Now that you can test sources, there is only one more step necessary
to test arbitrary sinks and decorators from the command line.  A sink
requires data to consume so some common sources used to generate test
data include synthetic datasets (<code class="literal">asciisynth</code>), the console
(<code class="literal">console</code>), or files (<code class="literal">text</code>).</p><p>For example, you can use a synthetic source to generate 1000 events,
each of 100 "random" bytes data to the console. You could use a text
source to read a file like /etc/services, or you could use the console
as a source and interactively enter lines of text as events:</p><pre class="screen">$ flume node_nowatch -1 -s -n dump -c 'dump: asciisynth(1000,100) | console;'
$ flume node_nowatch -1 -s -n dump -c 'dump: text("/etc/services") | console;'
$ flume node_nowatch -1 -s -n dump -c 'dump: console | console;'</pre><p>You can also use decorators on the sinks you specify.  For example,
you could rate limit the amount of data that pushed from the source to
sink by inserting a delay decorator.  In this case, the <span class="emphasis"><em>delay</em></span>
decorator waits 100ms before sending each synthesized event to the
console.</p><pre class="screen">$ flume node_nowatch -1 -s -n dump -c 'dump: asciisynth(1000,100) | { delay(100) =&gt; console};'</pre><p>Using the command line, you can send events via the direct best-effort
(BE) or disk-failover (DFO) agents.  The example below uses the
<code class="literal">console</code> source so you can interactively generate data to send in BE
and DFO mode to collectors.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Flume node_nowatch must be used when piping data in to a Flume
node&#8217;s console source.  The watchdog program does not forward stdin.</p></td></tr></table></div><pre class="screen">$ flume node_nowatch -1 -s -n dump -c 'dump: console | agentBESink("collectorHost");'
$ flume node_nowatch -1 -s -n dump -c 'dump: console | agentDFOSink("collectorHost");'</pre><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>Since these nodes are executed with configurations entered at
the command line and never contact the master, they cannot use the
automatic chains or logical node translations.  Currently, the
acknowledgements used in E2E mode go through the master piggy-backed
on node-to-master heartbeats.  Since this mode does not heartbeat, E2E
mode should not be used.</p></td></tr></table></div><p>Console sources are useful because we can pipe data into Flume
directly.  The next example pipes data from a program into Flume which
then delivers it.</p><pre class="screen">$ &lt;external process&gt; | flume node_nowatch -1 -s -n foo -c 'foo:console|agentBESink("collector");'</pre><p>Ideally, you could write data to a named pipe and just have Flume read
data from a named pipe using <code class="literal">text</code> or <code class="literal">tail</code>.  Unfortunately, this
version of Flume&#8217;s <code class="literal">text</code> and <code class="literal">tail</code> are not currently compatible with
named pipes in a Linux environment.  However, you could pipe data to a
Flume node listening on the stdin console:</p><pre class="screen">$ tail -f namedpipe | flume node_nowatch -1 -s -n foo -c 'foo:console|agentBESink;'</pre><p>Or you can use the exec source to get its output data:</p><pre class="screen">$ flume node_nowatch -1 -s -n bar -c 'bar:exec("cat pipe")|agentBESink;'</pre></div><div class="section" title="2.3. Monitoring nodes"><div class="titlepage"><div><div><h3 class="title"><a name="_monitoring_nodes"></a>2.3. Monitoring nodes</h3></div></div></div><p>While outputting data to a console or to a logfile is an effective way
to verify data transmission, Flume nodes provide a way to monitor the
state of sources and sinks.  This can be done by looking at the node&#8217;s
report page.  By default, you can navigate your web browser to the
nodes TCP 35862 port.  (<a class="ulink" href="http://node:35862/" target="_top">http://node:35862/</a>)</p><p>This page shows counter information about all of the logical nodes on
the physical node as well as some basic machine metric information.</p><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>If you have multiple physical nodes on a single machine, there
may be a port conflict.  If you have the auto-find port option on
(<code class="literal">flume.node.http.autofindport</code>), the physical node will increment the
port number until it finds a free port it can bind to.</p></td></tr></table></div></div></div><div class="section" title="3. Using Flume Agents for Apache 2.x Web Server Logging"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_using_flume_agents_for_apache_2_x_web_server_logging"></a>3. Using Flume Agents for Apache 2.x Web Server Logging</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_if_you_are_using_centos_red_hat_apache_servers">3.1. If you are using CentOS / Red Hat Apache servers</a></span></dt><dt><span class="section"><a href="#_if_you_are_using_ubuntu_servers_apache_servers">3.2. If you are using Ubuntu servers Apache servers</a></span></dt><dt><span class="section"><a href="#_getting_log_entries_from_piped_log_files">3.3. Getting log entries from Piped Log files</a></span></dt><dt><span class="section"><a href="#_using_piped_logs">3.4. Using piped logs</a></span></dt></dl></div><p>To connect Flume to Apache 2.x servers, you will need to</p><p># Configure web log file permissions</p><p># Tail the web logs or use piped logs to enable Flume to get data from
  the web server.</p><p>This section will step through basic setup on default Ubuntu Lucid and
default CentOS 5.5 installations.  Then it will describe various ways
of integrating Flume.</p><div class="section" title="3.1. If you are using CentOS / Red Hat Apache servers"><div class="titlepage"><div><div><h3 class="title"><a name="_if_you_are_using_centos_red_hat_apache_servers"></a>3.1. If you are using CentOS / Red Hat Apache servers</h3></div></div></div><p>By default, CentOS&#8217;s Apache writes weblogs to files owned by <code class="literal">root</code>
and in group <code class="literal">adm</code> in 0644 (rw-r&#8212;r--) mode.  Flume is run as the
<code class="literal">flume</code> user, so the Flume node is able to read the logs.</p><p>Apache on CentOS/Red Hat servers defaults to writing logs to two
files:</p><pre class="screen">/var/log/httpd/access_log
/var/log/httpd/error_log</pre><p>The simplest way to gather data from these files is to tail the files
by configuring Flume nodes to use Flume&#8217;s <code class="literal">tail</code> source:</p><pre class="screen">tail("/var/log/httpd/access_log")
tail("/var/log/httpd/error_log")</pre></div><div class="section" title="3.2. If you are using Ubuntu servers Apache servers"><div class="titlepage"><div><div><h3 class="title"><a name="_if_you_are_using_ubuntu_servers_apache_servers"></a>3.2. If you are using Ubuntu servers Apache servers</h3></div></div></div><p>By default, Ubuntu writes weblogs to files owned by <code class="literal">root</code> and in
group <code class="literal">adm</code> in 0640 (rw-r-----) mode.  Flume is run as the <code class="literal">flume</code>
user and by default will <span class="strong"><strong>not</strong></span> be able to tread the files.  One
approach to allow the <code class="literal">flume</code> user to read the files is to add it to
the <code class="literal">adm</code> group.</p><p>Apache servers on Ubuntu defaults to writing logs to three files:</p><pre class="screen">/var/log/apache2/access.log
/var/log/apache2/error.log
/var/log/apache2/other_vhosts_access.log</pre><p>The simplest way to gather data from these files is by configuring
Flume nodes to use Flume&#8217;s <code class="literal">tail</code> source:</p><pre class="screen">tail("/var/log/apache2/access.log")
tail("/var/log/apache2/error.log")
tail("/var/log/apache2/other_vhosts_access.log")</pre></div><div class="section" title="3.3. Getting log entries from Piped Log files"><div class="titlepage"><div><div><h3 class="title"><a name="_getting_log_entries_from_piped_log_files"></a>3.3. Getting log entries from Piped Log files</h3></div></div></div><p>The Apache 2.x&#8217;s documentation
(<a class="ulink" href="http://httpd.apache.org/docs/2.2/logs.html" target="_top">http://httpd.apache.org/docs/2.2/logs.html</a>) describes using piped
logfile with the <code class="literal">CustomLog</code> directive.  Their example uses
<code class="literal">rotatelogs</code> to periodically write data to new files with a given
prefix.  Here are some example directives that could be in the
<code class="literal">httpd.conf</code>/<code class="literal">apache2.conf</code> file.</p><pre class="screen">LogFormat "%h %l %u %t \"%r\" %&gt;s %b" common
CustomLog "|/usr/sbin/rotatelogs /var/log/apache2/foo_access_log 3600" common</pre><div class="tip" title="Tip" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Tip"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Tip]" src="images/tip.png"></td><th align="left">Tip</th></tr><tr><td align="left" valign="top"><p>In Ubuntu Lucid these directives are in
<code class="literal">/etc/apache2/sites-available/default</code>. In CentOS 5.5, these
directives are in <code class="literal">/etc/httpd/conf/httpd.conf</code>.</p></td></tr></table></div><p>These directives configure Apache to write log files in
<code class="literal">/var/log/apache2/foo_access_log.xxxxx</code> every hour (3600 seconds)
using the "common" log format.</p><p>You can use Flume&#8217;s <code class="literal">tailDir</code> source to read all files without
modifing the Apache settings:</p><pre class="screen">tailDir("/var/log/apache2/", "foo_access_log.*")</pre><p>The first argument is the directory, and then the second is a regex
that should match against the file name.  <code class="literal">tailDir</code> will watch the dir
and tail all files that have matching file names.</p></div><div class="section" title="3.4. Using piped logs"><div class="titlepage"><div><div><h3 class="title"><a name="_using_piped_logs"></a>3.4. Using piped logs</h3></div></div></div><p>Instead of writing data to disk and then having Flume read it, you can
have Flume ingest data directly from Apache.  To do so, modify the web
server&#8217;s parameters and use its piped log feature by adding some
directives to Apache&#8217;s configuration:</p><pre class="screen">CustomLog "|flume node_nowatch -1 -s -n apache -c \'apache:console|agentBESink(\"collector\");\'" common</pre><pre class="screen">CustomLog "|flume node_nowatch -1 -s -n apache -c \'apache:console|agentDFOSink(\"collector\");\'" common</pre><div class="warning" title="Warning" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Warning"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Warning]" src="images/warning.png"></td><th align="left">Warning</th></tr><tr><td align="left" valign="top"><p>By default, CentOS does not have the Java required by the
Flume node in user <code class="literal">root</code>'s path.  You can use <code class="literal">alternatives</code> to
create a managed symlink in <code class="literal">/usr/bin/</code> for the java executable.</p></td></tr></table></div><p>Using piped logs can be more efficient, but is riskier because Flume
can deliver messages without saving on disk.  Doing this, however,
increases the probabiltiy of event loss.  From a security point of
view, this Flume node instance runs as Apache&#8217;s user which is often
<code class="literal">root</code> according to the Apache manual.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>You could configure the one-shot mode node to deliver data
directly to a collector.  This can only be done at the best effort or
disk-failover level.</p></td></tr></table></div><p>The prior examples use Flume nodes in one-shot mode which runs without
contacting a master.  Unfortunately, it means that one-shot mode
cannot directly use the automatic chains or the end-to-end (E2E)
reliablity mode.  This is because the automatic chains are generated
by the master and because E2E mode currently delivers acknowledgements
through the master.</p><p>However, you can have a one-shot Flume node deliver data to a Flume
local node daemon where the reliable E2E mode can be used.  In this
setup we would have the following Apache directive:</p><pre class="screen">CustomLog "|flume node_nowatch -1 -s -n apache -c \'apache:console|agentBESink(\"localhost\", 12345);\'" common</pre><p>Then you can have a Flume node setup to listen with the following
configuration:</p><pre class="screen">node : rpcSource(12345) | agentE2ESink("collector");</pre><p>Since this daemon node is connected to the master, it can use the
auto*Chains.</p><pre class="screen">node : rpcSource(12345) | autoE2EChain;</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>End-to-end mode attempts to ensure the deliver of the data that
enters the E2E sink.  In this one-shot-node to reliable-node scenario,
data is not safe it gets to the E2E sink.  However, since this is a
local connection, it should only fail when the machine or processes
fails.  The one-shot node can be set to disk failover (DFO) mode in
order to reduce the chance of message loss if the daemon node&#8217;s
configuration changes.</p></td></tr></table></div></div></div><div class="section" title="4. Flume Agents for Syslog data"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_flume_agents_for_syslog_data"></a>4. Flume Agents for Syslog data</h2></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_tailing_files">4.1. Tailing files</a></span></dt><dt><span class="section"><a href="#_delivering_syslog_events_via_sockets">4.2. Delivering Syslog events via sockets</a></span></dt><dd><dl><dt><span class="section"><a href="#_configuring_literal_syslogd_literal">4.2.1. Configuring <code class="literal">syslogd</code></a></span></dt><dt><span class="section"><a href="#_configuring_literal_rsyslog_literal">4.2.2. Configuring <code class="literal">rsyslog</code></a></span></dt><dt><span class="section"><a href="#_configuring_literal_syslog_ng_literal">4.2.3. Configuring <code class="literal">syslog-ng</code></a></span></dt></dl></dd></dl></div><p><code class="literal">syslog</code> is the standard unix single machine logging service.  Events
are generally emitted as lines with a time stamp, "facility" type,
priority, and message.  Syslog can be configured to send data to
remote destinations.  The default syslog remote delivery was
originally designed to provide best effort delivery service.  Today,
there are several more advanced syslog services that deliver messages
with improved reliability (TCP connections with memory buffering on
failure).  The reliability guarantees however are one hop and weaker
than Flume&#8217;s more reliable delivery mechanism.</p><p>This section describes collecting syslog data using two methods.  The
first part describes a file tailing approach.  The latter parts
describe syslog system configuration guidance that enables directly
feeding Flume&#8217;s <code class="literal">syslog*</code> sources.</p><div class="section" title="4.1. Tailing files"><div class="titlepage"><div><div><h3 class="title"><a name="_tailing_files"></a>4.1. Tailing files</h3></div></div></div><p>The quickest way to record syslog messages is to tail syslog generated
log files.  These files generally live in <code class="literal">/var/log</code>.</p><p>Some examples include:</p><pre class="screen">/var/log/auth.log
/var/log/messages
/var/log/syslog
/var/log/user.log</pre><p>These files could be tailed by Flume nodes with tail sources:</p><pre class="screen">tail("/var/log/auth.log")
tail("/var/log/messages")
tail("/var/log/syslog")
tail("/var/log/user.log")</pre><p>Depending on your system configuration, there may be permissions
issues when accessing these files from the Flume node process.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Red Hat/CentOS systems default to writing log files owned by
root, in group root, and with 0600 (-rw-------) permissions. Flume
could be run as root, but this is not advised because Flume can be
remotely configured to execute arbitrary programs.</p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>Ubuntu systems default to writing logs files owned by syslog, in
group adm, and with 0640 (-rw-r-----) permissions.  By adding the user
"flume" to group "adm", a Flume node running user "flume" should be
able to read the syslog generated files.</p></td></tr></table></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>When tailing files, the time when the event is read is used as the
time stamp.</p></td></tr></table></div></div><div class="section" title="4.2. Delivering Syslog events via sockets"><div class="titlepage"><div><div><h3 class="title"><a name="_delivering_syslog_events_via_sockets"></a>4.2. Delivering Syslog events via sockets</h3></div></div></div><div class="toc"><dl><dt><span class="section"><a href="#_configuring_literal_syslogd_literal">4.2.1. Configuring <code class="literal">syslogd</code></a></span></dt><dt><span class="section"><a href="#_configuring_literal_rsyslog_literal">4.2.2. Configuring <code class="literal">rsyslog</code></a></span></dt><dt><span class="section"><a href="#_configuring_literal_syslog_ng_literal">4.2.3. Configuring <code class="literal">syslog-ng</code></a></span></dt></dl></div><p>The original syslog listens to the <code class="literal">/dev/log</code> named pipe, and can be
configured to listen on UDP port
514. (<a class="ulink" href="http://tools.ietf.org/search/rfc5424" target="_top">http://tools.ietf.org/search/rfc5424</a>). More advanced versions
(rsyslog, syslog-ng) can send and recieve over TCP and may do
in-memory queuing/buffering. For example, syslog-ng and rsyslog can
optionally use the default UDP port 514 or use TCP port 514 for better
recovery options.</p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><table border="0" summary="Note"><tr><td rowspan="2" align="center" valign="top" width="25"><img alt="[Note]" src="images/note.png"></td><th align="left">Note</th></tr><tr><td align="left" valign="top"><p>By default only superusers can listen on on UDP/TCP ports 514.
Unix systems usually only allow ports &lt;1024 to be bound by
superusers. While Flume can run as superuser, from a security stance
this is not advised.  The examples provide directions to route to the
user-bindable port 5140.</p></td></tr></table></div><p>For debugging syslog configurations, you can just use <span class="emphasis"><em>flume dump</em></span>
with syslog sources.  This command outputs received syslog data to the
console.  To test if syslog data is coming in to the proper port you
can run this command from the command line:</p><pre class="screen">$ flume dump 'syslogUdp(5140)'</pre><p>This will dump all incoming events to the console.</p><p>If you are satisfied with your connection, you can have a Flume node
run on the machine configure its sink for the reliability level you
desire.</p><p>Using a <code class="literal">syslog*</code> Flume source will save the entire line of event
data, use the timestamp found in the original data, extract a <code class="literal">host</code>,
and attempt to extract a service from the syslog line.  All of these
map to a Flume event&#8217;s fields except for <code class="literal">service</code> so this is added as
extra metadata field to each event (this is a convention with
syslog defined in RFC).</p><p>So, a syslog entry whose body is this:</p><pre class="screen">Sep 14 07:57:24 blitzwing dhclient: bound to 192.168.126.212 -- renewal in 710 seconds.</pre><p>will have the Flume event body:</p><pre class="screen">Sep 14 07:57:24 blitzwing dhclient: bound to 192.168.126.212 -- renewal in 710 seconds.</pre><p>The event will also translated the "Sep 14 07:57:24" date+time data so
that it will be bucketable.  Since this date does not have a year, it
assumes the current year and since it has no timezone it assumes the
local timezone.  The host field should be "blitzwing", and the
optional "service" metadata field will contain "dhclient".</p><div class="section" title="4.2.1. Configuring syslogd"><div class="titlepage"><div><div><h4 class="title"><a name="_configuring_literal_syslogd_literal"></a>4.2.1. Configuring <code class="literal">syslogd</code></h4></div></div></div><p>The original syslog is <code class="literal">syslogd</code>.  It is configured by an
<code class="literal">/etc/syslog.conf</code> file.  Its format is fairly simple.</p><p>Syslog recieves messages and then sends to out to different facilities
that have associated names
(<a class="ulink" href="http://tools.ietf.org/search/rfc5424#section-6.2" target="_top">http://tools.ietf.org/search/rfc5424#section-6.2</a>).</p><p>The <code class="literal">/etc/syslog.conf</code> file essentially contains lists of facilities
and "actions".  These "actions" are destinations such as regular
files, but can also be named pipes, consoles, or remote machines.  One
can specify a remote machine by prefixing an <span class="emphasis"><em>@</em></span> symbol in front the
destination host machine.  If no port is specified, events are sent
via UDP port 514.</p><p>The example below specifies delivery to machine localhost on port
5140.</p><pre class="screen">user.*     @localhost:5140</pre><p>A Flume node daemon running on this machine would have a <code class="literal">syslogUdp</code>
source listening for new log data.</p><pre class="screen">host-syslog : syslogUdp(5140) | autoE2EChain ;</pre></div><div class="section" title="4.2.2. Configuring rsyslog"><div class="titlepage"><div><div><h4 class="title"><a name="_configuring_literal_rsyslog_literal"></a>4.2.2. Configuring <code class="literal">rsyslog</code></h4></div></div></div><p><code class="literal">rsyslog</code> is a more advanced drop-in replacement for syslog and the
default syslog system used by Ubuntu systems.  It supports basic
filtering, best effort delivery, and queuing for handling one-hop
downstream failures.</p><p><code class="literal">rsyslog</code> actually extends the syslog configuration file format.
Similar to regular <code class="literal">syslogd</code> you can send data to a remote machine on
listening on UDP port 514 (standard syslog port).</p><pre class="screen">*.*   @remotehost</pre><p>Moreover, <code class="literal">rsyslog</code> also allows you to use the more reliable TCP
protocol to send data to a remote host listening on TCP port 514.  In
<code class="literal">rsyslog</code> configurations, an <span class="emphasis"><em>@@</em></span> prefix dictates the use of TCP.</p><pre class="screen">*.*  @@remotehost</pre><p>Similarly, you can also append a suffix port number to have it deliver
to a particular port.  In this example, events are delivered to
localhost TCP port 5140.</p><pre class="screen">*.*  @@localhost:5140</pre><p>Assuming you have a Flume node daemon running on the local host, you
can capture syslog data by adding a logical node with the following
configuration:</p><pre class="screen">host-syslog : syslogTcp(5140) | autoE2EChain ;</pre></div><div class="section" title="4.2.3. Configuring syslog-ng"><div class="titlepage"><div><div><h4 class="title"><a name="_configuring_literal_syslog_ng_literal"></a>4.2.3. Configuring <code class="literal">syslog-ng</code></h4></div></div></div><p>Syslog-ng is another common replacement for the default syslog logging
system.  Syslog-ng has a different configuration file format but
essentially gives the operator the ability to send syslog data from
different facilities to different remote destinations.  TCP or UDP can
be used.</p><p>Here is an example of modifications to a <code class="literal">syslog-ng.conf</code> (often found
in <code class="literal">/etc/syslog-ng/</code>) file.</p><pre class="screen">## set up logging to loghost (which is flume)
destination loghost {
        tcp("localhost" port(5140));
};

# send everything to loghost, too
log {
        source(src);
        destination(loghost);
};</pre><p>Assuming you have a Flume node daemon running on the local host, you
can capture syslog data by adding a logical node with the following
configuration:</p><pre class="screen">host-syslog : syslogTcp(5140) | autoE2EChain ;</pre></div></div></div><div class="section" title="5. Logging Scribe Events to a Flume Agent"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="_logging_scribe_events_to_a_flume_agent"></a>5. Logging Scribe Events to a Flume Agent</h2></div></div></div><p>Flume can emulate a downstream node for applications that log using
Scribe.  Scribe is Facebook&#8217;s open source log aggregation framework.
It has a simple API and uses Thrift as its core network transport
mechanism.  Flume uses the same Thrift IDL file and can listen for
data provided by Scribe sources.</p><p>Scribe comes with an example application called <code class="literal">scribe_cat</code>.  It acts
like the unix <code class="literal">cat</code> program but sends data to a downstream host with a
specified "category".  Scribe by default uses TCP port 1463.</p><p>You can configure a Flume node to listen for incoming Scribe traffic
by creating a logical node that uses the <code class="literal">scribe</code> source.  We can then
assign an arbitrary sink to the node.  In the example below, the
Scribe nodes receives events, send its events to both the console and
an automatically-assigned end-to-end agent which delivers the events
downstream to its collector pools.</p><pre class="screen">scribe: scribe | [console, autoE2EChain];</pre><p>Assuming that this node communicates with the master, and that the
node is on <code class="literal">localhost</code>, we can use the <code class="literal">scribe_cat</code> program to send
data to Flume.</p><pre class="screen">$ echo "foo" | scribe_cat localhost testcategory</pre><p>When the Scribe source accepts the Scribe events, it converts Scribe&#8217;s
category information into a new Flume event metadata entry, and then
delivers the event to its sinks.  Since Scribe does not include time
metadata, the timestamp of the created Flume event will be the arrival
time of the Scribe event.</p><pre class="literallayout">     ______
    / ___//_  ______  ____
   / /_/ / / / /    \/ __/
  / __/ / /_/ / / / / __/
 / / /_/\____/_/_/_/\__/
/_/ Distributed Log Collection.</pre></div></div><script type="text/javascript">
     var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
     document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script><script type="text/javascript">
     try{
        var pageTracker = _gat._getTracker("UA-2275969-4");
        pageTracker._setDomainName(".cloudera.com");
        pageTracker._trackPageview();
     } catch(err) {}
  </script><div class="footer-text"><span align="center"><a href="index.html"><img src="images/home.png" alt="Documentation Home"></a></span><br>
  This document was built from Flume source available at
  <a href="http://github.com/cloudera/flume">http://github.com/cloudera/flume</a>.
  </div></body></html>
