////////////////////
Licensed to Cloudera, Inc. under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  Cloudera, Inc. licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
////////////////////

== Integrating Flume with your Data Sources

.WARNING 
******************** 
This section is incomplete.
********************

Flume's source interface is designed to be simple yet powerful and enable logging 
of all kinds of data -- from unstructured blobs of byte, semi-structured blobs 
with structured metadata, to completely structured data.

////
Issues: language neutrality, reliability, push vs pull, one shot vs 
continuous. 
////

In this section we describe some of the basic mechanisms that can be used to 
pull in data.  Generally, this approach has three flavors. *Pushing* data to 
Flume, having Flume *polling* for data, or *embedding* Flume or Flume 
components into an application.

These mechanisms have different trade-offs -- based on the semantics of the 
operation.

Also, some sources can be *one shot* or *continuous* sources.


=== Push Sources

+syslogTcp+, +syslogUdp+ :: wire-compatibility with syslog, and syslog-ng 
logging protocols.

+scribe+ :: wire-compatibility with the scribe log collection system.

=== Polling Sources

+tail+, +multitail+ :: watches a file(s) for appends.

+exec+ :: This is good for extracting custom data by using existing programs.

+poller+ :: We can gather information from Flume nodes themselves.

=== Embedding Sources

WARNING: these features are incomplete.

+log4j+

+simple client library+


 // move this to gathering data from sources

=== Logging via log4j Directly

//// 
WARNING: These instructions are out of date and currently untested.

Modify hadoop-daemon.sh so that it includes Flume.

Places where log4j mentioned:

---- 
bin/hadoop-daemon.sh    -- defaults to INFO,DRFA 
conf/hadoop-env.sh      -- can be set but can cause perms issues 
bin/hadoop              -- defaults to INFO,console 
conf/log4j.properties   -- loggers are defined here, but the default root logger is ignored

src/java/..../TaskRunner.java -- INFO,TLA 
---- 
////

==== Example of Logging Hadoop Jobs 
//// 
For jobs initiated by the user, the easiest mechanism to enable Flume logging is to modify conf/hadoop-env.sh to include:

export HADOOP_ROOT_LOGGER=INFO,flume,console 
////

==== Logging Hadoop Daemons 
//// 
To log events generated by Hadoop's daemons (tasktracker, jobtracker, datanode, secondarynamenode, namenode), modify bin/hadoop-daemon.sh so that HADOOP_ROOT_LOGGER is set to  

export HADOOP_ROOT_LOGGER=INFO,flume,DRFA

TODO (jon) this doesn't seem to working right now --  need to figure out why. 
////
